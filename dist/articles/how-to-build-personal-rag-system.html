<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to Build a Personal RAG System: A Weekend Project | The Full Stack</title>
    <meta name="description" content="A step-by-step guide to building your own local, private context engine using Ollama, FAISS, and Python. No cloud APIs required.">
    <!-- Schema.org Article markup -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "How to Build a Personal RAG System: A Weekend Project",
      "description": "A step-by-step guide to building your own local, private context engine using Ollama, FAISS, and Python. No cloud APIs required.",
      "author": {
        "@type": "Person",
        "name": "Chris O'Hara",
        "url": "https://chrisohara.com",
        "jobTitle": "VP of Product Marketing",
        "worksFor": {
          "@type": "Organization",
          "name": "SAP"
        }
      },
      "datePublished": "2026-02-14",
      "dateModified": "2026-02-14",
      "publisher": {
        "@type": "Organization",
        "name": "The Full Stack",
        "url": "https://thefullstack.chrisohara.com"
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://thefullstack.chrisohara.com/how-to-build-personal-rag-system.html"
      }
    }
    </script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,300;0,6..72,400;0,6..72,500;0,6..72,600;1,6..72,300;1,6..72,400;1,6..72,500&family=DM+Sans:ital,wght@0,300;0,400;0,500;0,600;1,300;1,400&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #FDFBF7;
            --bg-card: #FFFFFF;
            --text-primary: #1A1A1A;
            --text-secondary: #6B6B6B;
            --text-tertiary: #9A9A9A;
            --accent: #C05626;
            --accent-hover: #A84920;
            --border: #E8E4DE;
            --border-light: #F0ECE6;
            --tag-bg: #F5F1EB;
            --tag-text: #7A7168;
            --serif: 'Newsreader', Georgia, serif;
            --sans: 'DM Sans', -apple-system, sans-serif;
            --mono: 'JetBrains Mono', monospace;
            --max-width: 720px;
            --wide-width: 960px;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        html { scroll-behavior: smooth; }
        body { font-family: var(--sans); background: var(--bg); color: var(--text-primary); line-height: 1.6; -webkit-font-smoothing: antialiased; }
        
        nav { position: sticky; top: 0; z-index: 100; background: rgba(253, 251, 247, 0.92); backdrop-filter: blur(12px); border-bottom: 1px solid var(--border-light); }
        .nav-inner { max-width: var(--wide-width); margin: 0 auto; padding: 0 24px; height: 60px; display: flex; align-items: center; justify-content: space-between; }
        .nav-brand { font-family: var(--mono); font-size: 15px; font-weight: 500; letter-spacing: -0.02em; color: var(--text-primary); text-decoration: none; }
        .nav-brand .slash { color: var(--accent); }
        .nav-links { display: flex; gap: 32px; align-items: center; }
        .nav-links a { font-size: 14px; font-weight: 400; color: var(--text-secondary); text-decoration: none; transition: color 0.2s; }
        .nav-links a:hover { color: var(--text-primary); }
        
        .article-header { max-width: var(--max-width); margin: 0 auto; padding: 64px 24px 32px; }
        .back-link { display: inline-flex; align-items: center; gap: 6px; font-size: 14px; color: var(--text-tertiary); text-decoration: none; margin-bottom: 32px; transition: color 0.2s; }
        .back-link:hover { color: var(--text-primary); }
        .article-meta { display: flex; align-items: center; gap: 12px; margin-bottom: 16px; flex-wrap: wrap; }
        .article-date { font-size: 13px; color: var(--text-tertiary); }
        .article-tag { font-size: 11px; font-weight: 500; text-transform: uppercase; letter-spacing: 0.06em; color: var(--accent); background: rgba(192, 86, 38, 0.08); padding: 2px 8px; border-radius: 3px; }
        .article-title { font-family: var(--serif); font-size: 42px; font-weight: 400; line-height: 1.15; letter-spacing: -0.03em; margin-bottom: 16px; }
        .article-subtitle { font-family: var(--serif); font-size: 20px; font-weight: 300; font-style: italic; color: var(--text-secondary); line-height: 1.5; }
        
        .article-body { max-width: var(--max-width); margin: 0 auto; padding: 0 24px 80px; font-family: var(--serif); font-size: 18px; line-height: 1.8; }
        .article-body p { margin-bottom: 24px; }
        .article-body strong { font-weight: 600; }
        .article-body em { font-style: italic; }
        .article-body a { color: var(--accent); text-decoration: none; border-bottom: 1px solid rgba(192, 86, 38, 0.3); transition: border-color 0.2s; }
        .article-body a:hover { border-color: var(--accent); }
        .article-body h2 { font-family: var(--serif); font-size: 28px; font-weight: 500; margin: 48px 0 20px; line-height: 1.3; }
        .article-body h3 { font-family: var(--serif); font-size: 22px; font-weight: 500; margin: 36px 0 16px; line-height: 1.35; }
        .article-body ul, .article-body ol { margin: 0 0 24px 24px; }
        .article-body li { margin-bottom: 12px; }
        .article-body blockquote { border-left: 3px solid var(--accent); padding: 16px 24px; margin: 32px 0; background: var(--tag-bg); border-radius: 0 8px 8px 0; }
        .article-body blockquote p { margin-bottom: 0; font-style: italic; color: var(--text-secondary); }
        
        .code-block { background: #1e1e1e; color: #d4d4d4; padding: 20px 24px; border-radius: 8px; margin: 24px 0; overflow-x: auto; font-family: var(--mono); font-size: 14px; line-height: 1.6; }
        .code-block code { background: none; padding: 0; }
        .code-inline { background: #f0f0f0; padding: 2px 6px; border-radius: 4px; font-family: var(--mono); font-size: 15px; }
        
        .step-box { background: var(--bg-card); border: 1px solid var(--border); padding: 24px; margin: 32px 0; border-radius: 12px; }
        .step-number { display: inline-block; width: 32px; height: 32px; background: var(--accent); color: white; border-radius: 50%; text-align: center; line-height: 32px; font-family: var(--sans); font-weight: 600; font-size: 14px; margin-right: 12px; }
        .step-title { font-family: var(--sans); font-size: 18px; font-weight: 600; margin-bottom: 12px; display: inline; }
        .step-content { margin-top: 16px; }
        .step-content p { margin-bottom: 16px; }
        .step-content p:last-child { margin-bottom: 0; }
        
        .prereq-box { background: linear-gradient(135deg, #eff6ff 0%, #f0f9ff 100%); border-left: 3px solid #3b82f6; padding: 24px; margin: 32px 0; border-radius: 0 12px 12px 0; }
        .prereq-box h4 { font-family: var(--sans); font-size: 16px; font-weight: 600; color: #1e40af; margin-bottom: 12px; }
        .prereq-box ul { margin: 0 0 0 20px; font-family: var(--sans); font-size: 15px; }
        .prereq-box li { margin-bottom: 8px; }
        
        .warning-box { background: linear-gradient(135deg, #fef2f2 0%, #fff7ed 100%); border-left: 3px solid #f59e0b; padding: 24px; margin: 32px 0; border-radius: 0 12px 12px 0; }
        .warning-box h4 { font-family: var(--sans); font-size: 16px; font-weight: 600; color: #b45309; margin-bottom: 8px; }
        .warning-box p { font-family: var(--sans); font-size: 15px; margin: 0; }
        
        .success-box { background: linear-gradient(135deg, #ecfdf5 0%, #f0fdf4 100%); border-left: 3px solid #059669; padding: 24px; margin: 32px 0; border-radius: 0 12px 12px 0; }
        .success-box h4 { font-family: var(--sans); font-size: 16px; font-weight: 600; color: #059669; margin-bottom: 8px; }
        .success-box p { font-family: var(--sans); font-size: 15px; margin: 0; }
        
        .architecture-diagram { background: var(--bg-card); border: 1px solid var(--border); padding: 32px; margin: 32px 0; border-radius: 12px; text-align: center; }
        .architecture-diagram img { max-width: 100%; height: auto; }
        .diagram-text { font-family: var(--mono); font-size: 13px; color: var(--text-secondary); margin-top: 16px; }
        
        table { width: 100%; border-collapse: collapse; margin: 24px 0; font-family: var(--sans); font-size: 15px; }
        th { background: #f5f5f5; text-align: left; padding: 12px 16px; font-weight: 600; border-bottom: 2px solid var(--border); }
        td { padding: 12px 16px; border-bottom: 1px solid var(--border-light); }
        tr:hover { background: #fafafa; }
        
        .time-estimate { display: inline-block; background: #dbeafe; color: #1e40af; padding: 4px 12px; border-radius: 20px; font-family: var(--sans); font-size: 13px; font-weight: 500; margin-left: 8px; }
        
        footer { border-top: 1px solid var(--border); padding: 32px 24px; text-align: center; }
        .footer-inner { max-width: var(--wide-width); margin: 0 auto; display: flex; justify-content: space-between; align-items: center; }
        .footer-brand { font-family: var(--mono); font-size: 13px; color: var(--text-tertiary); }
        .footer-brand .slash { color: var(--accent); }
        .footer-links { display: flex; gap: 20px; }
        .footer-links a { font-size: 13px; color: var(--text-tertiary); text-decoration: none; transition: color 0.2s; }
        .footer-links a:hover { color: var(--text-primary); }
        
        @media (max-width: 640px) {
            .article-title { font-size: 32px; }
            .article-header { padding: 40px 20px 24px; }
            .article-body { padding: 0 20px 60px; }
            .code-block { padding: 16px; font-size: 12px; }
            .footer-inner { flex-direction: column; gap: 12px; }
        }
    </style>
</head>
<body>
    <nav>
        <div class="nav-inner">
            <a href="/" class="nav-brand">the<span class="slash">/</span>full<span class="slash">/</span>stack</a>
            <div class="nav-links">
                <a href="/">Essays</a>
                <a href="/digests/">Daily Digest</a>
                <a href="/about.html">About</a>
            </div>
        </div>
    </nav>
    
    <header class="article-header">
        <a href="../index.html" class="back-link">← Back to all articles</a>
        <div class="article-meta">
            <span class="article-date">February 2026</span>
            <span class="article-tag">AI</span>
            <span class="article-tag">Tutorial</span>
            <span class="article-tag">RAG</span>
        </div>
        <h1 class="article-title">How to Build a Personal RAG System</h1>
        <p class="article-subtitle">A step-by-step guide to building your own local, private context engine. No cloud APIs, no monthly fees, no data leaving your machine.</p>
    </header>
    
    <article class="article-body">
        <p>This guide is based on <a href="olivier-duvelleroy-rag-brain.html">Olivier Duvelleroy's NEXUS project</a>—a personal RAG system he built in a single weekend to solve the file-limit problem with AI tools. If you haven't read that story, start there for context on why this matters.</p>
        
        <p>Here, we'll walk through exactly how to build your own version.</p>
        
        <div class="prereq-box">
            <h4>What You'll Need</h4>
            <ul>
                <li><strong>A computer with 16GB+ RAM</strong> (the model runs locally)</li>
                <li><strong>~10GB free disk space</strong> (for the model and index)</li>
                <li><strong>Basic comfort with the terminal</strong> (you'll copy/paste commands)</li>
                <li><strong>Documents to index</strong> (PDFs, Word docs, text files)</li>
                <li><strong>2-4 hours</strong> (mostly waiting for downloads)</li>
            </ul>
        </div>
        
        <h2>The Architecture</h2>
        
        <p>Before we start, let's understand what we're building:</p>
        
        <div class="architecture-diagram">
            <pre style="text-align: left; display: inline-block;">
┌─────────────────────────────────────────────────────────────┐
│                      YOUR DOCUMENTS                         │
│            (PDFs, Word docs, text files, etc.)              │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│                    CHUNKING & EMBEDDING                     │
│         Split into paragraphs → Convert to vectors          │
│              (sentence-transformers library)                │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│                     FAISS VECTOR INDEX                      │
│       Fast similarity search across all your chunks         │
└─────────────────────────┬───────────────────────────────────┘
                          │
              ┌───────────┴───────────┐
              │                       │
              ▼                       ▼
┌─────────────────────┐   ┌───────────────────────────────────┐
│    YOUR QUESTION    │   │         RETRIEVED CHUNKS          │
│                     │──▶│    (Top 5-10 most relevant)       │
└─────────────────────┘   └───────────────────┬───────────────┘
                                              │
                                              ▼
                          ┌───────────────────────────────────┐
                          │         LOCAL LLM (Ollama)        │
                          │   Question + Context → Answer     │
                          │           (Qwen3:8b)              │
                          └───────────────────────────────────┘
            </pre>
            <p class="diagram-text">NEXUS Architecture: Index once, query instantly</p>
        </div>
        
        <p>The key insight: <strong>retrieval is fast</strong> (a few seconds), while <strong>generation is slower</strong> (1-2 minutes on local hardware). This is fine because you're trading speed for privacy and unlimited context.</p>
        
        <h2>Step 1: Install Ollama <span class="time-estimate">~10 min</span></h2>
        
        <div class="step-box">
            <div class="step-content">
                <p>Ollama is a local AI runtime that makes it dead simple to run open-source models. Think of it as "Docker for LLMs."</p>
                
                <p><strong>On Mac:</strong></p>
                <div class="code-block"><code>curl -fsSL https://ollama.com/install.sh | sh</code></div>
                
                <p><strong>On Windows:</strong> Download from <a href="https://ollama.com/download" target="_blank">ollama.com/download</a></p>
                
                <p><strong>On Linux:</strong></p>
                <div class="code-block"><code>curl -fsSL https://ollama.com/install.sh | sh</code></div>
                
                <p>Verify it's working:</p>
                <div class="code-block"><code>ollama --version</code></div>
            </div>
        </div>
        
        <h2>Step 2: Pull a Model <span class="time-estimate">~20 min</span></h2>
        
        <div class="step-box">
            <div class="step-content">
                <p>We'll use Qwen3:8b—a capable model that runs well on consumer hardware. It's 4-bit quantized, meaning it's compressed to use less memory while maintaining quality.</p>
                
                <div class="code-block"><code>ollama pull qwen3:8b</code></div>
                
                <p>This downloads about 5GB. Go grab coffee.</p>
                
                <p>Test it:</p>
                <div class="code-block"><code>ollama run qwen3:8b "What is RAG in AI?"</code></div>
                
                <p>You should see a response about Retrieval-Augmented Generation. If your fan starts spinning, that's normal—your CPU is doing the work.</p>
            </div>
        </div>
        
        <div class="warning-box">
            <h4>Performance Note</h4>
            <p>Local inference is slower than cloud APIs. Expect 1-2 minutes per response on a typical laptop. This is the tradeoff for privacy and no API costs. If you need speed, you can swap in a cloud API later—the architecture supports both.</p>
        </div>
        
        <h2>Step 3: Set Up Python Environment <span class="time-estimate">~15 min</span></h2>
        
        <div class="step-box">
            <div class="step-content">
                <p>Create a clean Python environment for the project:</p>
                
                <div class="code-block"><code># Create project directory
mkdir nexus && cd nexus

# Create virtual environment
python -m venv venv

# Activate it
source venv/bin/activate  # Mac/Linux
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install langchain langchain-community sentence-transformers faiss-cpu pypdf python-docx</code></div>
                
                <p>What we're installing:</p>
                <ul>
                    <li><strong>langchain:</strong> Orchestration framework for LLM pipelines</li>
                    <li><strong>sentence-transformers:</strong> Creates embeddings from text</li>
                    <li><strong>faiss-cpu:</strong> Facebook's vector similarity search</li>
                    <li><strong>pypdf, python-docx:</strong> Document parsers</li>
                </ul>
            </div>
        </div>
        
        <h2>Step 4: Create the Indexing Script <span class="time-estimate">~30 min</span></h2>
        
        <div class="step-box">
            <div class="step-content">
                <p>This script reads your documents, splits them into chunks, creates embeddings, and stores them in a FAISS index.</p>
                
                <p>Create <code class="code-inline">index_documents.py</code>:</p>
                
                <div class="code-block"><code>import os
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Configuration
DOCS_PATH = "./documents"  # Put your docs here
INDEX_PATH = "./faiss_index"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

def load_documents(docs_path):
    """Load all documents from the specified directory."""
    documents = []
    
    for file_path in Path(docs_path).rglob("*"):
        if file_path.suffix.lower() == ".pdf":
            loader = PyPDFLoader(str(file_path))
        elif file_path.suffix.lower() in [".docx", ".doc"]:
            loader = Docx2txtLoader(str(file_path))
        elif file_path.suffix.lower() in [".txt", ".md"]:
            loader = TextLoader(str(file_path))
        else:
            continue
            
        try:
            documents.extend(loader.load())
            print(f"Loaded: {file_path.name}")
        except Exception as e:
            print(f"Error loading {file_path.name}: {e}")
    
    return documents

def main():
    print("Loading documents...")
    documents = load_documents(DOCS_PATH)
    print(f"Loaded {len(documents)} document pages")
    
    print("Splitting into chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    print(f"Created {len(chunks)} chunks")
    
    print("Creating embeddings (this takes a while)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    
    print("Building FAISS index...")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    
    print(f"Saving index to {INDEX_PATH}...")
    vectorstore.save_local(INDEX_PATH)
    
    print("Done! Index ready for queries.")

if __name__ == "__main__":
    main()</code></div>
            </div>
        </div>
        
        <h2>Step 5: Create the Query Script <span class="time-estimate">~30 min</span></h2>
        
        <div class="step-box">
            <div class="step-content">
                <p>This script loads the index, retrieves relevant chunks for your question, and sends them to the local LLM for an answer.</p>
                
                <p>Create <code class="code-inline">query.py</code>:</p>
                
                <div class="code-block"><code>import sys
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

INDEX_PATH = "./faiss_index"
MODEL_NAME = "qwen3:8b"
TOP_K = 5  # Number of chunks to retrieve

# Custom prompt for grounded answers
PROMPT_TEMPLATE = """Use the following context to answer the question. 
If you cannot answer based on the context, say so. 
Always cite which documents support your answer.

Context:
{context}

Question: {question}

Answer:"""

def main():
    if len(sys.argv) < 2:
        print("Usage: python query.py 'Your question here'")
        sys.exit(1)
    
    question = " ".join(sys.argv[1:])
    
    print("Loading index...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    vectorstore = FAISS.load_local(
        INDEX_PATH, embeddings, allow_dangerous_deserialization=True
    )
    
    print("Connecting to Ollama...")
    llm = Ollama(model=MODEL_NAME, temperature=0.1)
    
    print("Creating retrieval chain...")
    prompt = PromptTemplate(
        template=PROMPT_TEMPLATE,
        input_variables=["context", "question"]
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": TOP_K}),
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
    
    print(f"\nQuestion: {question}\n")
    print("Thinking... (this takes 1-2 minutes)\n")
    
    result = qa_chain({"query": question})
    
    print("=" * 60)
    print("ANSWER:")
    print("=" * 60)
    print(result["result"])
    
    print("\n" + "=" * 60)
    print("SOURCES:")
    print("=" * 60)
    for doc in result["source_documents"]:
        source = doc.metadata.get("source", "Unknown")
        print(f"- {source}")

if __name__ == "__main__":
    main()</code></div>
            </div>
        </div>
        
        <h2>Step 6: Index Your Documents <span class="time-estimate">Varies</span></h2>
        
        <div class="step-box">
            <div class="step-content">
                <p>Create a <code class="code-inline">documents</code> folder and add your files:</p>
                
                <div class="code-block"><code>mkdir documents
# Copy your PDFs, Word docs, and text files here</code></div>
                
                <p>Run the indexing script:</p>
                
                <div class="code-block"><code>python index_documents.py</code></div>
                
                <p>This will take a while depending on how many documents you have. For 100 documents, expect 10-20 minutes.</p>
            </div>
        </div>
        
        <div class="success-box">
            <h4>Checkpoint</h4>
            <p>You should now have a <code>faiss_index</code> folder containing your vectorized knowledge base. This only needs to be rebuilt when you add new documents.</p>
        </div>
        
        <h2>Step 7: Query Your Knowledge Base</h2>
        
        <div class="step-box">
            <div class="step-content">
                <p>Now the fun part. Ask questions:</p>
                
                <div class="code-block"><code>python query.py "What did customers say about pricing in our research?"

python query.py "Summarize the main themes from the Gartner reports"

python query.py "Find quotes about AI adoption challenges"</code></div>
                
                <p>The system will:</p>
                <ol>
                    <li>Convert your question to an embedding (instant)</li>
                    <li>Find the 5 most relevant chunks (instant)</li>
                    <li>Send question + context to the local LLM (1-2 min)</li>
                    <li>Return a grounded answer with sources</li>
                </ol>
            </div>
        </div>
        
        <h3>Real Example: What NEXUS Actually Outputs</h3>
        
        <p>Here's a real example from Olivier's system. He asked NEXUS to find relevant quotes for each chapter of a book he's writing about the "context problem" in enterprise data:</p>
        
        <div style="background: var(--tag-bg); border: 1px solid var(--border); border-radius: 12px; padding: 24px; margin: 24px 0; overflow-x: auto;">
            <p style="font-weight: 600; color: var(--text-primary); margin: 0 0 8px;">Part 6 — Context problem</p>
            <p style="font-style: italic; color: var(--text-secondary); margin: 0 0 20px;">Data exists, but meaning is lost across systems</p>
            
            <table style="width: 100%; border-collapse: collapse; font-size: 0.9rem; font-family: var(--sans);">
                <thead>
                    <tr style="border-bottom: 2px solid var(--border);">
                        <th style="text-align: left; padding: 10px; width: 100px;">Chapter</th>
                        <th style="text-align: left; padding: 10px;">Expanded quote</th>
                        <th style="text-align: left; padding: 10px; width: 100px;">Reference</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="border-bottom: 1px solid var(--border-light);">
                        <td style="padding: 12px 10px; font-weight: 600;">Setup</td>
                        <td style="padding: 12px 10px; font-style: italic; color: var(--text-secondary);">"If a user is spending their time in Outlook or in CRM, how do you take that insight and make sure it gets to them in that application? Otherwise, it just stays disconnected from action."</td>
                        <td style="padding: 12px 10px; color: var(--text-tertiary); font-size: 0.85rem;">Qual Transcript 2</td>
                    </tr>
                    <tr style="border-bottom: 1px solid var(--border-light);">
                        <td style="padding: 12px 10px; font-weight: 600;">Exploration</td>
                        <td style="padding: 12px 10px; font-style: italic; color: var(--text-secondary);">"All these tools have siloed, contextual metadata. A lot of the understanding sits in one person's head. When they leave or switch roles, that context disappears."</td>
                        <td style="padding: 12px 10px; color: var(--text-tertiary); font-size: 0.85rem;">Qual Transcript 1</td>
                    </tr>
                    <tr style="border-bottom: 1px solid var(--border-light);">
                        <td style="padding: 12px 10px; font-weight: 600;">Practical</td>
                        <td style="padding: 12px 10px; font-style: italic; color: var(--text-secondary);">"We are very clear that Confluence is the source for documentation. If you want to understand definitions or logic, there is one place to go. That consistency matters when people start self-serving."</td>
                        <td style="padding: 12px 10px; color: var(--text-tertiary); font-size: 0.85rem;">Qual Transcript 5</td>
                    </tr>
                    <tr>
                        <td style="padding: 12px 10px; font-weight: 600;">Synthesis</td>
                        <td style="padding: 12px 10px; font-style: italic; color: var(--text-secondary);">"AI models don't understand business rules or why decisions were made. Without decision memory and context, agents will act quickly but incorrectly."</td>
                        <td style="padding: 12px 10px; color: var(--text-tertiary); font-size: 0.85rem;">Qual Transcript 5</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <p><strong>This is the magic:</strong> In 30 seconds, NEXUS retrieved the most relevant quotes from 40+ interview transcripts, matched them to book chapters, and cited the exact source. No manual searching. No missed documents.</p>
        
        <h2>What's Next</h2>
        
        <p>This is a working foundation. Here's how to extend it:</p>
        
        <table>
            <thead>
                <tr>
                    <th>Improvement</th>
                    <th>Difficulty</th>
                    <th>Impact</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Add a simple web UI (Gradio/Streamlit)</td>
                    <td>Easy</td>
                    <td>Much better UX</td>
                </tr>
                <tr>
                    <td>Swap in GPT-4 for faster inference</td>
                    <td>Easy</td>
                    <td>10x faster responses</td>
                </tr>
                <tr>
                    <td>Add incremental indexing</td>
                    <td>Medium</td>
                    <td>Faster updates</td>
                </tr>
                <tr>
                    <td>Support images/diagrams (vision model)</td>
                    <td>Hard</td>
                    <td>Much richer context</td>
                </tr>
                <tr>
                    <td>Add memory across sessions</td>
                    <td>Medium</td>
                    <td>Conversational queries</td>
                </tr>
            </tbody>
        </table>
        
        <h2>The Hybrid Future</h2>
        
        <p>Olivier's insight: this doesn't have to be all-local forever. The architecture supports a <strong>hybrid approach</strong>:</p>
        
        <ul>
            <li><strong>Local retrieval</strong> for governance (your documents never leave your machine)</li>
            <li><strong>Optional cloud inference</strong> for speed/quality when content is non-sensitive</li>
        </ul>
        
        <p>You control the tradeoff. That's the point.</p>
        
        <blockquote>
            <p>"At no time should we be afraid of diving into the details these days. AI tools can guide you through implementation; you are mostly limited by your curiosity, creativity, and intent."</p>
            <p>— Olivier Duvelleroy</p>
        </blockquote>
        
        <p>You now have your own context engine. What will you ask it?</p>
    </article>
    
    <footer>
        <div class="footer-inner">
            <span class="footer-brand">the<span class="slash">/</span>full<span class="slash">/</span>stack</span>
            <div class="footer-links">
                <a href="../index.html">Essays</a>
                <a href="https://linkedin.com/in/christopherohara" target="_blank">LinkedIn</a>
                <a href="https://www.chrisohara.com" target="_blank">chrisohara.com</a>
            </div>
        </div>
    </footer>
</body>
</html>
